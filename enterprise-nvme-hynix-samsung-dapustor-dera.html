<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=description content="Drive throughput with a queue depth of one is usually not advertised, but almost every latency or consistency metric reported on a spec sheet is measured at QD1 and usually for 4kB transfers. When the drive only has one command to work on at a time, there's nothing to get in the way of it"><meta name=author content="Jenniffer Sheldon"><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/base16/css/style.css type=text/css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type=text/css><link rel=alternate href=./index.xml type=application/rss+xml title=WinkDash><title>Performance at QD1 - Enterprise NVMe Round-Up 2: SK Hynix, Samsung, DapuStor and DERA - WinkDash</title></head><body><header><div class="container clearfix"><a class=path href=./index.html>[WinkDash]</a>
<span class=caret># _</span><div class=right></div></div></header><div class=container><main role=main class=article><article class=single itemscope itemtype=http://schema.org/BlogPosting><div class=meta><span class=key>published on</span>
<span class=val><time itemprop=datePublished datetime=2024-09-15>September 15, 2024</time></span>
<span class=key>in</span>
<span class=val><a href=./categories/blog>blog</a></span></div><h1 class=headline itemprop=headline>Performance at QD1 - Enterprise NVMe Round-Up 2: SK Hynix, Samsung, DapuStor and DERA</h1><section class=body itemprop=articleBody><h2>QD1 Random Read Performance</h2><p>Drive throughput with a queue depth of one is usually not advertised, but almost every latency or consistency metric reported on a spec sheet is measured at QD1 and usually for 4kB transfers. When the drive only has one command to work on at a time, there's nothing to get in the way of it offering its best-case access latency. Performance at such light loads is absolutely not what most of these drives are made for, but they have to make it through the easy tests before we move on to the more realistic challenges.</p><p><img alt="4kB Random Read QD1" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-rr.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>Random read performance at QD1 is mostly determined by the inherent latency of the underlying storage medium. Since most of these SSDs are using 64+ layer 3D TLC, they're all on a fairly even footing. The SK hynix PE6011 is the slowest of the new NVMe drives while the Dapu Haishen3 and Samsung PM1725a are among the fastest.</p><table align=center border=0 cellpadding=10 cellspacing=1 width=678><tbody readability=1><tr><td align=center colspan=10><img alt="4kB Random Read QD1 (Power Efficiency)" id=qd1_rr_eff src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-rr-eff.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=2><td align=center class=tlgrey onclick='document.getElementById("qd1_rr_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-rr-eff.png"'>Power Efficiency in kIOPS/W</td><td align=center class=tlgrey onclick='document.getElementById("qd1_rr_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-rr-pow.png"'>Average Power in W</td></tr></tbody></table><p>The drives with big 16-channel controllers have the worst power efficiency at low load, because they idle in the 6-7W range. The DERA SSDs, Samsung PM1725 and Memblaze PBlaze5 are all in the same ballpark. The SK hynix PE6011 is a clear step up, and the Dapu Haishen3s are the most efficient of the new drives on this test. The SATA drives and Samsung's low-power 983 DCT still have higher efficiency ratings because they're under 2W during this test.</p><p><img alt="4kB Random Read QD1 QoS" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-rr-qos.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>Flipping the numbers around to look at latency instead of IOPS, we see that the DERA drives seem to have surprisingly high tail latencies comparable to the old Micron SATA drive and far in excess of any of the other NVMe drives. The rest of the new NVMe drives in our collection have great QoS out to four 9s.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=678><tbody readability=2.5><tr class=tgrey><td align=center colspan=10><img alt id=rr_size src=https://cdn.statically.io/img/images.anandtech.com/doci/15491/rr-size-pe6011-1920.png width=650 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=5><td align=center></td></tr></tbody></table><p>There aren't any surprises when looking at random read performance across a range of block sizes. All of the new NVMe drives have constant IOPS for block sizes of 4kB and smaller, and for larger block sizes IOPS decreases but throughput grows significantly.</p><h2>QD1 Random Write Performance</h2><p><img alt="4kB Random Write QD1" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-rw.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>Random write performance at QD1 is mostly a matter of delivering the data into the SSD's cache and getting back an acknowledgement; the performance of the NAND flash itself doesn't factor in much until the drive is busier with a higher queue depth. The exception here is the Optane SSD, which doesn't have or need a DRAM cache. Between the fastest and slowest flash-based NVMe SSDs here we're only looking at about a 30% difference. The SK hynix PE6011 and Samsung PM1725a are a bit on the slow side, while the DERA SSDs are among the fastest.</p><table align=center border=0 cellpadding=10 cellspacing=1 width=678><tbody readability=1><tr><td align=center colspan=10><img alt="4kB Random Write QD1 (Power Efficiency)" id=qd1_rw_eff src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-rw-eff.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=2><td align=center class=tlgrey onclick='document.getElementById("qd1_rw_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-rw-eff.png"'>Power Efficiency in kIOPS/W</td><td align=center class=tlgrey onclick='document.getElementById("qd1_rw_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-rw-pow.png"'>Average Power in W</td></tr></tbody></table><p>Power draw during this test is generally higher than for the QD1 random read test, but the pattern of bigger SSD controllers being less efficient still mostly holds true. The Dapu Haishen3 and SK hynix PE6011 are the most efficient of our new NVMe drives, and are also helped some by their lower capacity: the 2TB models don't have to spend as much power on keeping DRAM and NAND chips awake.</p><p><img alt="4kB Random Write QD1 QoS" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-rw-qos.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>All the drives start to show elevated tail latency when we go out to four 9s, but the SK hynix PE6011 and Samsung PM1725a also have issues at the 99th percentile level (as does the Intel P4510). The Dapu Haishen3 drives have the best QoS scores on this performance even though their average latency is a few microseconds slower than the fastest flash-based SSDs in this batch.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=678><tbody readability=2.5><tr class=tgrey><td align=center colspan=10><img alt id=rw_size src=https://cdn.statically.io/img/images.anandtech.com/doci/15491/rw-size-pe6011-1920.png width=650 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=5><td align=center></td></tr></tbody></table><p>Looking at random write performance for different block sizes reveals major differences between drives. Everything has obviously been optimized to offer peak IOPS with 4kB block size (except for the Optane SSD). However, several drives do so at the expense of vastly lower performance on sub-4kB block sizes. The DapuStor Haishen3 and DERA SSDs join the Memblaze PBlaze5 on the list of drives that maybe shouldn't even offer the option of operating with 512-byte sector sizes. For those drives, IOPS falls by a factor of 4-5x and they seem to be bottlenecked by doing a read-modify-write cycle in order to support small block writes.</p><h2>QD1 Sequential Read Performance</h2><p><img alt="128kB Sequential Read QD1" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-sr.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>When performing sequential reads of 128kB blocks, QD1 isn't enough for any of these drives to really stretch their legs. Unlike consumer SSDs, most of these drives seem to be doing little or no readahead caching, which is probably a reasonable decision for heavily multi-user environments where IO is less predictable. It does lead to lackluster performance numbers, with none of our new drives breaking 1GB/s. The DERA SSDs are fastest of the new bunch, but are only half as fast on this test as the Intel P4510 or Samsung 983 DCT.</p><table align=center border=0 cellpadding=10 cellspacing=1 width=678><tbody readability=1><tr><td align=center colspan=10><img alt="128kB Sequential Read QD1 (Power Efficiency)" id=qd1_sr_eff src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-sr-eff.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=2><td align=center class=tlgrey onclick='document.getElementById("qd1_sr_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-sr-eff.png"'>Power Efficiency in MB/s/W</td><td align=center class=tlgrey onclick='document.getElementById("qd1_sr_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-sr-pow.png"'>Average Power in W</td></tr></tbody></table><p>Even though we're starting to get up to non-trivial throughput with this test, the power efficiency scores are still dominated by the baseline idle power draw of these SSDs. The 16-channel drives are mostly in the 8-9W range (DERA, Samsung PM1725a) while the 8-channel drives are around half that. The DapuStor Haishen3 drives are the most efficient of our new drives, but are still clearly a ways behind the Intel P4510 and Samsung 983 DCT that are much faster on this test.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=678><tbody readability=2.5><tr class=tgrey><td align=center colspan=10><img alt id=sr_size src=https://cdn.statically.io/img/images.anandtech.com/doci/15491/sr-size-pe6011-1920.png width=650 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=5><td align=center></td></tr></tbody></table><p>All of the new NVMe drives in our collection are still showing a lot of performance growth by the time the block size test reaches 1MB reads. At that point, they've all at least caught up with the handful of other drives that performed very well on the QD1/128kB sequential read test, but it's clear that they need either a higher queue depth or even larger block sizes in order to make the most of their theoretical throughput.</p><h2>QD1 Sequential Write Performance</h2><p><img alt="128kB Sequential Write QD1" src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-sw.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>A few different effects are at play during our QD1 sequential write test. The drives were preconditioned with a few full drive writes before the test, so they're at or near steady-state when this test begins. This leads to the general pattern of larger drives or drives with more overprovisioning performing better, because they can more easily free up a block to accept new writes. However, at QD1 the drives are getting a bit of idle time when waiting on the host system to deliver the next write command, and that results in poor link utilization and fairly low top speeds. It also compresses the spread of scores slightly compared to what the spec sheets indicate we'll see at high queue depths.</p><p>The DapuStor Haishen3 drives stand out as the best performers in the 2TB class; they break the pattern of better performance from bigger drives and are performing on par with the 8TB class drives with comparable overprovisioning ratios.</p><table align=center border=0 cellpadding=10 cellspacing=1 width=678><tbody readability=1><tr><td align=center colspan=10><img alt="128kB Sequential Write QD1 (Power Efficiency)" id=qd1_sw_eff src=https://cdn.statically.io/img/images.anandtech.com/graphs/graph15491/qd1-sw-eff.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=2><td align=center class=tlgrey onclick='document.getElementById("qd1_sw_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-sw-eff.png"'>Power Efficiency in MB/s/W</td><td align=center class=tlgrey onclick='document.getElementById("qd1_sw_eff").src="http://images.anandtech.com/graphs/graph15491/qd1-sw-pow.png"'>Average Power in W</td></tr></tbody></table><p>The 1.6TB DapuStor Haishen3 H3100 stands out as the most efficient flash-based NVMe SSD on this test, by a fairly wide margin. Its QD1 sequential write performance is similar to the 8TB drives with 16-channel controllers, but the Haishen3 H3100 is also tied for lowest power consumption among the NVMe drives: just under 7W compared to a maximum of over 18W for the 8TB DERA D5437. The Haishen3 H3000's efficiency score is more in line with the rest of the competition, because its lower overprovisioning ratio forces it to spend quite a bit more power on background flash management even at this low queue depth.</p><table align=center border=0 cellpadding=0 cellspacing=1 width=678><tbody readability=2.5><tr class=tgrey><td align=center colspan=10><img alt id=sw_size src=https://cdn.statically.io/img/images.anandtech.com/doci/15491/sw-size-pe6011-1920.png width=650 style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></td></tr><tr readability=5><td align=center></td></tr></tbody></table><p>In contrast to our random write block size test, for sequential writes extremely poor small-block write performance seems to be the norm rather than the exception; most of these drives don't take kindly to sub-4kB writes. Increasing block sizes past 128kB up to at least 1MB doesn't help the sequential write performance of these drives; in order to hit the speeds advertised on the spec sheets, we need to go beyond QD1.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH52gJhqZp6mpJq%2Fsb7IrJxmpqaism602KegsWWjlrq0wc2gZJ2ZoKrAtbvRZpueqpFkgg%3D%3D</p></section></article></main></div><footer><div class=container><span class=copyright>&copy; 2024 WinkDash - <a rel=license href=http://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a></span></div></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>